{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDsVMGiVgSq2"
      },
      "source": [
        "## Классификация FashionMNIST\n",
        "\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/s/girafe_ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3isBRG6PgSq6"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import json\n",
        "import os\n",
        "import copy\n",
        "import re\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import clear_output\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iOhCgLcVYeip"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "def get_predictions(model, eval_data, step=10):\n",
        "\n",
        "    predicted_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, len(eval_data), step):\n",
        "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "def get_accuracy(model, data_loader):\n",
        "    predicted_labels = []\n",
        "    real_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            y_predicted = model(batch[0].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "            real_labels.append(batch[1])\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    real_labels = torch.cat(real_labels)\n",
        "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
        "    return accuracy_score\n",
        "\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDW6QvzQYeip"
      },
      "source": [
        "Загрузите файл `hw_overfitting_data_dict.npy` (ссылка есть на странице с заданием), он понадобится для генерации посылок. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zFwHHNROYeiq",
        "outputId": "ffa20ff7-2748-4f21-e235-9e413ee41361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-12 14:24:09--  https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict [following]\n",
            "--2025-04-12 14:24:10--  https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6272446 (6.0M) [application/octet-stream]\n",
            "Saving to: ‘hw_overfitting_data_dict.npy’\n",
            "\n",
            "hw_overfitting_data 100%[===================>]   5.98M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-04-12 14:24:10 (79.6 MB/s) - ‘hw_overfitting_data_dict.npy’ saved [6272446/6272446]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict -O hw_overfitting_data_dict.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Kml1jWsOYeiq"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeA6Q5-CgSq7"
      },
      "source": [
        "Вернемся к задаче распознавания простых изображений, рассмотренной ранее. Но теперь будем работать с набором данных [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). В данном задании воспользуемся всем датасетом целиком.\n",
        "\n",
        "__Ваша первая задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 88.5\\%$ на тестовой выборке.__\n",
        "\n",
        "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбукам с предыдущих занятий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B8bIFid_Yeiq"
      },
      "outputs": [],
      "source": [
        "CUDA_DEVICE_ID = 0  # change if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nPG1KbQAgl8b"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = (\n",
        "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "aYcL28OsgSq8",
        "outputId": "bbf7cbe5-d793-4054-9c63-2f32043c4ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.4MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 208kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.90MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Image label: 7')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "train_fmnist_data = FashionMNIST(\n",
        "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "test_fmnist_data = FashionMNIST(\n",
        "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_fmnist_data, batch_size=32, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_fmnist_data, batch_size=32, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "random_batch = next(iter(train_data_loader))\n",
        "_image, _label = random_batch[0][0], random_batch[1][0]\n",
        "plt.figure()\n",
        "plt.imshow(_image.reshape(28, 28))\n",
        "plt.title(f\"Image label: {_label}\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6jWRv1rgSq8"
      },
      "source": [
        "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 88.5% accuracy.\n",
        "\n",
        "__Внимание, ваша модель должна быть представлена именно переменной `model_task_1`. На вход ей должен приходить тензор размерностью (1, 28, 28).__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BcyEFX-RgSq8"
      },
      "outputs": [],
      "source": [
        "class FmnistCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FmnistCNN, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, padding=2),  # Добавил padding\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Добавил padding\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # Изменил на более стандартные параметры\n",
        "        )\n",
        "\n",
        "        # Добавим слой для автоматического вычисления размерности\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Временный линейный слой для определения размерности\n",
        "        self.linear_block = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(64*7*7, 64),  # Начальное значение (может потребовать корректировки)\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Если размерность не совпадает, выведем её для отладки\n",
        "        if self.linear_block[1].in_features != x.size(1):\n",
        "            print(f\"Ошибка размерности! Ожидалось {self.linear_block[1].in_features}, получено {x.size(1)}\")\n",
        "            # Динамически создаём правильный линейный слой\n",
        "            self.linear_block[1] = nn.Linear(x.size(1), 64).to(x.device)\n",
        "\n",
        "        x = self.linear_block(x)\n",
        "        return x\n",
        "\n",
        "model_task_1 = FmnistCNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAoLV4dkoy5M"
      },
      "source": [
        "Не забудьте перенести модель на выбранный `device`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Xas9SIXDoxvZ",
        "outputId": "c4cbe292-966a-4acf-cfd5-8efcb0c6f2b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FmnistCNN(\n",
              "  (conv_block): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_block): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=3136, out_features=64, bias=True)\n",
              "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Dropout(p=0.5, inplace=False)\n",
              "    (5): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model_task_1.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pLRWysggSq9"
      },
      "source": [
        "Локальные тесты для проверки вашей модели доступны ниже:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qMQzo1ggSq9",
        "outputId": "2b92b946-a192-4d01-a8a1-9054997d41c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ошибка размерности! Ожидалось 3136, получено 12544\n",
            "Everything seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
        "\n",
        "try:\n",
        "    x = random_batch[0].to(device)\n",
        "    y = random_batch[1].to(device)\n",
        "\n",
        "    # compute outputs given inputs, both are variables\n",
        "    y_predicted = model_task_1(x)\n",
        "except Exception as e:\n",
        "    print(\"Something is wrong with the model\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
        "\n",
        "print(\"Everything seems fine!\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRmIPwIgSq9"
      },
      "source": [
        "Настройте параметры модели на обучающей выборке. Также рекомендуем поработать с `learning rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YJnU14bdnZa_",
        "outputId": "9eb91a18-037c-446d-f075-e135293a85c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.850971\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.666014\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.575238\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.523188\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.459657\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.495366\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.503357\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.374989\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.615105\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.467814\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.523030\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.428330\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.485396\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.475690\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.417036\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.252773\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.530451\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.294369\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.362380\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.250264\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.595797\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.274600\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.371815\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.288824\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.527857\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.314600\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.523294\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.347336\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.317991\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.309283\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.201571\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.339277\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.290965\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.675278\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.410522\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.285857\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.153575\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.517647\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.314097\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.348520\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.228844\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.289616\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.319167\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.231421\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.300692\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.303991\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.223484\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.246607\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.148273\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.121877\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.255538\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.306151\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.279133\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.348844\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.092752\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.391321\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.209638\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.214934\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.402624\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.187997\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.335833\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.266391\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.259839\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.175329\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.258112\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.463362\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.219073\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.372921\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.234617\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.272829\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.307037\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.288442\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.232247\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.120991\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.294268\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.295719\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.441500\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.229365\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.186206\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.449830\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.379723\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.320938\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.224337\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.066118\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.311725\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.330711\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.211483\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.256816\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.336231\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.613282\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.153458\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.159604\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.233937\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.128630\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.511209\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.153105\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.141175\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.287429\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.378596\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.135670\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.136236\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.390424\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.238377\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.510183\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.228160\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.175453\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.310994\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.200627\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.144425\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.211568\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.249945\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.281366\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.112558\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.099687\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.223185\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.340923\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.455998\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.225748\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.155183\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.461570\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.161583\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.212995\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.209649\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.063825\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.249027\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.316259\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.274175\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.164265\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.109068\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.242750\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.294859\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.480517\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.527892\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.186591\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.130335\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.065648\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.347891\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.372865\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.192698\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.124818\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.281480\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.193666\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.130423\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.126896\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.120864\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.206121\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.371079\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.222793\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.287882\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.162954\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.062255\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.103217\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.258046\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.267881\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.189587\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.323999\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.225216\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.299832\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.099389\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.168018\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.175918\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.177699\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.226296\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.180863\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.177547\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.233768\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.146609\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.213449\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.154530\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.070979\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.507169\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.286792\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.219012\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.421689\n",
            "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.067045\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.370284\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.117822\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.261007\n",
            "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.241974\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.407916\n",
            "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.203266\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.255131\n",
            "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.287310\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.175400\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.124204\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.448730\n",
            "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.265622\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.146542\n",
            "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.065595\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.394669\n",
            "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.259949\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.334198\n",
            "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.294814\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.202722\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.292905\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.170820\n",
            "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.147668\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.296646\n",
            "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.262913\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.339503\n",
            "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.120945\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.217752\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.164780\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.254031\n",
            "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.092366\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.188495\n",
            "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.391393\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.076273\n",
            "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.147114\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.278598\n",
            "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.169780\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.162754\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.203902\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.218452\n",
            "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.233395\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.345794\n",
            "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.105951\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.280589\n",
            "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.066005\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.271145\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.283737\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.113379\n",
            "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.117831\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.159451\n",
            "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.364295\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.160509\n",
            "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.092492\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.352983\n",
            "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.147146\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.079223\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.111354\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.108195\n",
            "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.186141\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.185246\n",
            "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.153730\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.089872\n",
            "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.263413\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.358824\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.114009\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.100443\n",
            "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.604793\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.168441\n",
            "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.257091\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.176060\n",
            "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.064232\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.217451\n",
            "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.149329\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.463646\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.265986\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.184464\n",
            "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.181322\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.263668\n",
            "Train Epoch: 15 [3200/60000 (5%)]\tLoss: 0.210042\n",
            "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.162817\n",
            "Train Epoch: 15 [9600/60000 (16%)]\tLoss: 0.143780\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.114848\n",
            "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 0.416862\n",
            "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.401304\n",
            "Train Epoch: 15 [22400/60000 (37%)]\tLoss: 0.043756\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.119465\n",
            "Train Epoch: 15 [28800/60000 (48%)]\tLoss: 0.198122\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.142947\n",
            "Train Epoch: 15 [35200/60000 (59%)]\tLoss: 0.246189\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.088159\n",
            "Train Epoch: 15 [41600/60000 (69%)]\tLoss: 0.462160\n",
            "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.098295\n",
            "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 0.383796\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.552759\n",
            "Train Epoch: 15 [54400/60000 (91%)]\tLoss: 0.037923\n",
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.109585\n",
            "Train Epoch: 16 [3200/60000 (5%)]\tLoss: 0.179798\n",
            "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.198030\n",
            "Train Epoch: 16 [9600/60000 (16%)]\tLoss: 0.202662\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.282853\n",
            "Train Epoch: 16 [16000/60000 (27%)]\tLoss: 0.274233\n",
            "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.331802\n",
            "Train Epoch: 16 [22400/60000 (37%)]\tLoss: 0.213774\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.051148\n",
            "Train Epoch: 16 [28800/60000 (48%)]\tLoss: 0.357569\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.306265\n",
            "Train Epoch: 16 [35200/60000 (59%)]\tLoss: 0.173690\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.105761\n",
            "Train Epoch: 16 [41600/60000 (69%)]\tLoss: 0.109346\n",
            "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.223515\n",
            "Train Epoch: 16 [48000/60000 (80%)]\tLoss: 0.505934\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.238457\n",
            "Train Epoch: 16 [54400/60000 (91%)]\tLoss: 0.058311\n",
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.221832\n",
            "Train Epoch: 17 [3200/60000 (5%)]\tLoss: 0.130818\n",
            "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.135581\n",
            "Train Epoch: 17 [9600/60000 (16%)]\tLoss: 0.319649\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.131518\n",
            "Train Epoch: 17 [16000/60000 (27%)]\tLoss: 0.308617\n",
            "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.153230\n",
            "Train Epoch: 17 [22400/60000 (37%)]\tLoss: 0.070749\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.062969\n",
            "Train Epoch: 17 [28800/60000 (48%)]\tLoss: 0.160105\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.220406\n",
            "Train Epoch: 17 [35200/60000 (59%)]\tLoss: 0.277381\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.188647\n",
            "Train Epoch: 17 [41600/60000 (69%)]\tLoss: 0.240186\n",
            "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.217986\n",
            "Train Epoch: 17 [48000/60000 (80%)]\tLoss: 0.126504\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.208115\n",
            "Train Epoch: 17 [54400/60000 (91%)]\tLoss: 0.187633\n",
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.081050\n",
            "Train Epoch: 18 [3200/60000 (5%)]\tLoss: 0.265375\n",
            "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.281426\n",
            "Train Epoch: 18 [9600/60000 (16%)]\tLoss: 0.174214\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.287805\n",
            "Train Epoch: 18 [16000/60000 (27%)]\tLoss: 0.147356\n",
            "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.155257\n",
            "Train Epoch: 18 [22400/60000 (37%)]\tLoss: 0.104549\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.130992\n",
            "Train Epoch: 18 [28800/60000 (48%)]\tLoss: 0.079719\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.263112\n",
            "Train Epoch: 18 [35200/60000 (59%)]\tLoss: 0.274998\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.128211\n",
            "Train Epoch: 18 [41600/60000 (69%)]\tLoss: 0.061647\n",
            "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.163192\n",
            "Train Epoch: 18 [48000/60000 (80%)]\tLoss: 0.079129\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.170915\n",
            "Train Epoch: 18 [54400/60000 (91%)]\tLoss: 0.274038\n",
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.236226\n",
            "Train Epoch: 19 [3200/60000 (5%)]\tLoss: 0.047975\n",
            "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.093408\n",
            "Train Epoch: 19 [9600/60000 (16%)]\tLoss: 0.151403\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.225341\n",
            "Train Epoch: 19 [16000/60000 (27%)]\tLoss: 0.142946\n",
            "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.191275\n",
            "Train Epoch: 19 [22400/60000 (37%)]\tLoss: 0.078480\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.202260\n",
            "Train Epoch: 19 [28800/60000 (48%)]\tLoss: 0.182509\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.072340\n",
            "Train Epoch: 19 [35200/60000 (59%)]\tLoss: 0.120182\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.298029\n",
            "Train Epoch: 19 [41600/60000 (69%)]\tLoss: 0.301270\n",
            "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.202209\n",
            "Train Epoch: 19 [48000/60000 (80%)]\tLoss: 0.020251\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.868516\n",
            "Train Epoch: 19 [54400/60000 (91%)]\tLoss: 0.347083\n",
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.064594\n",
            "Train Epoch: 20 [3200/60000 (5%)]\tLoss: 0.294314\n",
            "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.203790\n",
            "Train Epoch: 20 [9600/60000 (16%)]\tLoss: 0.246210\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.171593\n",
            "Train Epoch: 20 [16000/60000 (27%)]\tLoss: 0.302270\n",
            "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.243057\n",
            "Train Epoch: 20 [22400/60000 (37%)]\tLoss: 0.175537\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.149460\n",
            "Train Epoch: 20 [28800/60000 (48%)]\tLoss: 0.098195\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.067048\n",
            "Train Epoch: 20 [35200/60000 (59%)]\tLoss: 0.169766\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.192365\n",
            "Train Epoch: 20 [41600/60000 (69%)]\tLoss: 0.341301\n",
            "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.394476\n",
            "Train Epoch: 20 [48000/60000 (80%)]\tLoss: 0.120008\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.083958\n",
            "Train Epoch: 20 [54400/60000 (91%)]\tLoss: 0.219009\n",
            "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.147670\n",
            "Train Epoch: 21 [3200/60000 (5%)]\tLoss: 0.100485\n",
            "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 0.292213\n",
            "Train Epoch: 21 [9600/60000 (16%)]\tLoss: 0.180221\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.114878\n",
            "Train Epoch: 21 [16000/60000 (27%)]\tLoss: 0.239885\n",
            "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.162555\n",
            "Train Epoch: 21 [22400/60000 (37%)]\tLoss: 0.153442\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.108345\n",
            "Train Epoch: 21 [28800/60000 (48%)]\tLoss: 0.312648\n",
            "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 0.161545\n",
            "Train Epoch: 21 [35200/60000 (59%)]\tLoss: 0.370491\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.143784\n",
            "Train Epoch: 21 [41600/60000 (69%)]\tLoss: 0.167427\n",
            "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 0.147194\n",
            "Train Epoch: 21 [48000/60000 (80%)]\tLoss: 0.209589\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.115902\n",
            "Train Epoch: 21 [54400/60000 (91%)]\tLoss: 0.209416\n",
            "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.093268\n",
            "Train Epoch: 22 [3200/60000 (5%)]\tLoss: 0.162482\n",
            "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 0.188956\n",
            "Train Epoch: 22 [9600/60000 (16%)]\tLoss: 0.058056\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.269799\n",
            "Train Epoch: 22 [16000/60000 (27%)]\tLoss: 0.167702\n",
            "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.075690\n",
            "Train Epoch: 22 [22400/60000 (37%)]\tLoss: 0.105365\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.154552\n",
            "Train Epoch: 22 [28800/60000 (48%)]\tLoss: 0.057101\n",
            "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 0.285471\n",
            "Train Epoch: 22 [35200/60000 (59%)]\tLoss: 0.068948\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.187628\n",
            "Train Epoch: 22 [41600/60000 (69%)]\tLoss: 0.105071\n",
            "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 0.070927\n",
            "Train Epoch: 22 [48000/60000 (80%)]\tLoss: 0.075308\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.182162\n",
            "Train Epoch: 22 [54400/60000 (91%)]\tLoss: 0.266708\n",
            "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.213439\n",
            "Train Epoch: 23 [3200/60000 (5%)]\tLoss: 0.196134\n",
            "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 0.298211\n",
            "Train Epoch: 23 [9600/60000 (16%)]\tLoss: 0.247712\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.214877\n",
            "Train Epoch: 23 [16000/60000 (27%)]\tLoss: 0.086141\n",
            "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.092623\n",
            "Train Epoch: 23 [22400/60000 (37%)]\tLoss: 0.086194\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.153823\n",
            "Train Epoch: 23 [28800/60000 (48%)]\tLoss: 0.169026\n",
            "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 0.082801\n",
            "Train Epoch: 23 [35200/60000 (59%)]\tLoss: 0.171602\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.219808\n",
            "Train Epoch: 23 [41600/60000 (69%)]\tLoss: 0.097003\n",
            "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 0.215782\n",
            "Train Epoch: 23 [48000/60000 (80%)]\tLoss: 0.299829\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.090251\n",
            "Train Epoch: 23 [54400/60000 (91%)]\tLoss: 0.082600\n",
            "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.048665\n",
            "Train Epoch: 24 [3200/60000 (5%)]\tLoss: 0.324420\n",
            "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 0.162616\n",
            "Train Epoch: 24 [9600/60000 (16%)]\tLoss: 0.063608\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.091610\n",
            "Train Epoch: 24 [16000/60000 (27%)]\tLoss: 0.279656\n",
            "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.102993\n",
            "Train Epoch: 24 [22400/60000 (37%)]\tLoss: 0.118994\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.304832\n",
            "Train Epoch: 24 [28800/60000 (48%)]\tLoss: 0.135935\n",
            "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 0.094876\n",
            "Train Epoch: 24 [35200/60000 (59%)]\tLoss: 0.183424\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.077132\n",
            "Train Epoch: 24 [41600/60000 (69%)]\tLoss: 0.065462\n",
            "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 0.153371\n",
            "Train Epoch: 24 [48000/60000 (80%)]\tLoss: 0.080951\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.207418\n",
            "Train Epoch: 24 [54400/60000 (91%)]\tLoss: 0.092150\n",
            "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 0.147146\n",
            "Train Epoch: 25 [3200/60000 (5%)]\tLoss: 0.067812\n",
            "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 0.346498\n",
            "Train Epoch: 25 [9600/60000 (16%)]\tLoss: 0.212433\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.183015\n",
            "Train Epoch: 25 [16000/60000 (27%)]\tLoss: 0.315186\n",
            "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 0.376332\n",
            "Train Epoch: 25 [22400/60000 (37%)]\tLoss: 0.067426\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.249528\n",
            "Train Epoch: 25 [28800/60000 (48%)]\tLoss: 0.211511\n",
            "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 0.120199\n",
            "Train Epoch: 25 [35200/60000 (59%)]\tLoss: 0.115546\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.178138\n",
            "Train Epoch: 25 [41600/60000 (69%)]\tLoss: 0.248856\n",
            "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 0.077777\n",
            "Train Epoch: 25 [48000/60000 (80%)]\tLoss: 0.458409\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.244288\n",
            "Train Epoch: 25 [54400/60000 (91%)]\tLoss: 0.057186\n",
            "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 0.219866\n",
            "Training complete in 5m 7s\n",
            "Best val Acc: 0.000000\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 25\n",
        "batch_size_train = 32\n",
        "batch_size_test = 32\n",
        "learning_rate = 0.002\n",
        "momentum = 0.9\n",
        "log_interval = 100\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(params=model_task_1.parameters(), lr=0.001)\n",
        "\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_data_loader.dataset) for i in range(1, num_epoch + 1)]\n",
        "\n",
        "best_model_wts = copy.deepcopy(model_task_1.state_dict())\n",
        "best_acc = 0.0\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "for epoch in range(1, num_epoch + 1):\n",
        "    model_task_1.train()\n",
        "    for i, (images, labels) in enumerate(train_data_loader):\n",
        "        images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model_task_1(images)\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        if (i + 1)% log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, (i + 1) * len(images), len(train_data_loader.dataset),\n",
        "                100. * (i + 1) / len(train_data_loader), loss.data))\n",
        "            train_losses.append(loss.item())\n",
        "            train_counter.append((i*64) + ((epoch-1)*len(train_data_loader.dataset)))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val Acc: {:4f}'.format(best_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zce7gt1gSq-"
      },
      "source": [
        "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usswrWYOgSq-"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Xua3TVZHgSq-",
        "outputId": "c2cf4739-d304-4600-b62b-bc9c73eae736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on train set: 0.97552\n"
          ]
        }
      ],
      "source": [
        "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "l9KEKXBxgSq-",
        "outputId": "d07c8e1d-2403-4ed2-ff87-c94418f22386",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on test set: 0.9285\n"
          ]
        }
      ],
      "source": [
        "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oyhmMobgSq_"
      },
      "source": [
        "Проверка, что необходимые пороги пройдены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OAIrURCEgSq_"
      },
      "outputs": [],
      "source": [
        "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
        "assert (\n",
        "    train_acc_task_1 >= 0.905\n",
        "), \"Train accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3tq_T-jYeit"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_1`, а файл `hw_fmnist_data_dict.npy` находится в той же директории, что и ноутбук (он доступен в репозитории)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TX4_0v_TYeit",
        "outputId": "55535959-97c7-4cc6-e389-9952f3619067",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_fmnist_task_1.json`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_fmnist_data_dict.npy\"\n",
        "), \"Please, download `hw_fmnist_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"hw_fmnist_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict = {\n",
        "    \"train_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "    ),\n",
        "    \"test_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "    ),\n",
        "}\n",
        "\n",
        "with open(\"submission_dict_fmnist_task_1.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_fmnist_task_1.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfjyOk6MYeiu"
      },
      "source": [
        "### Сдача задания\n",
        "Сдайте сгенерированный файл в соответствующую задачу в соревновании, а именно:\n",
        "    \n",
        "* `submission_dict_fmnist_task_1.json` в задачу Separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtWnYAN_gSrA"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "21499ab2a6726e29f7050b76af0e9680227e613293d630ba279de7ebdfad9cae"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}